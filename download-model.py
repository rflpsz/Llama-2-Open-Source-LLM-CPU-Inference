from huggingface_hub import hf_hub_download
hf_hub_download(repo_id="TheBloke/Llama-2-7B-Chat-GGML", filename="llama-2-7b-chat.ggmlv3.q8_0.bin", output_path="/workspace/Llama-2-Open-Source-LLM-CPU-Inference/models/7B")
# hf_hub_download(repo_id="TheBloke/Llama-2-13B-chat-GGML", filename="llama-2-13b-chat.ggmlv3.q8_0.bin", output_path="/workspace/Llama-2-Open-Source-LLM-CPU-Inference/models/13B")